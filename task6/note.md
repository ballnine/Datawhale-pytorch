+ 批量归一化和残差网络

  learning rate:较大最后一层不易收敛，较小不易收敛。

  批量归一化，对模型某一层使用激活函数的输入进行归一化，使批量呈标准正态分布

  全连接层，每个输入特征的均方和方差；卷积层，每个通道计算均方和方差

  测试时，也需要均一化。训练时保存全局均值和方差，使用其。

  一般插在卷积和激活之间

  收敛更快，学习率可以设的大一点

  + resnet

  通过跨层连接解决梯度回传时变小。最底层输出不仅传给中间层，且与中间层相加进入最上层。
  
  块沿用全3*3卷积，在卷积和池化层间加入批量归一层加速训练。每次跨层连接跨过两层卷积。通过步幅为2的残差块在每个模块之间减小高和宽。

  + DenseNet

  相比resnet，把+改成concat,连接通道数。稠密层、通道层（控制通道不过打）

  过渡层：1*1卷积层减少通道数（除以2），步幅为2平均池化层减少高宽

  输出通道数=输入通道数+卷积层个数*卷积输出通道数

+ 凸优化

  优化——训练集损失小

  深度学习——测试集损失小

  + 局部最小值不是全局最小值（梯度下降时）

  + 鞍点

  + 梯度消失

  + 凸性

    凸函数无局部最小值

+ 梯度下降（优化算法）

  + 一维梯度下降

    沿梯度反方向移动自变量可以减小函数值

    学习率过小过大（局部最小值）

  + 自适应方法（自动调整学习率）

    牛顿法，考虑二阶导
  
  + 随机梯度下降

    复杂度O(n)-> O(1),对每一个样本计算梯度后都对样本更新

  + 动态学习率

    随着迭代次数改变学习率

