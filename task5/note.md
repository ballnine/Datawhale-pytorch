+ CNN基础

  + 二维卷积层

    参数包括卷积核和标量偏置

    `nn.Parameter`自动添加梯度，自动注册到模型参数中

    多输出通道：一个ci*hi*wi一个局部特征，多个核数组提取不同的特征

    1*1卷积核：不改变高宽基础上，改变通道数。把通道维当做特征，和全连接层等价。

+ LeNet

  全连接层缺点：相邻像素展开后位置远，参数过大

  卷积层通道数加大，保持特征不被丢失

  卷积神经网络就是含卷积层的网络。 LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。

  输入——卷积层（5*5卷积核、平均池化）——全连接隐藏层——输出层

+ 卷积神经网络进阶

  + AlexNet

    学习到特征超过手工设计的特征（11*11卷积核、5*5、3*3、最大池化）

    使用sigmod激活函数，引用dropout

    feature map高宽计算：⌊(x−kernel)/stride⌋+1

  + VGG(使用重复元素的网络)

    可以改变模型来适应，重复使用简单的基础块

    Block:数个相同的填充为1、窗口形状为的卷积层,接上一个步幅为2、窗口形状为的最大池化层。

  + NiN（网络中的网络）

    串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。

    使用1*1卷积层代替flatten、全连接层，最后使用全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。调整最后一个模块通道数使输出为类别数分类。

    + 1×1卷积核作用
      
      1.放缩通道数：通过控制卷积核的数量达到通道数的放缩。
      
      2.增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。
      
      3.计算参数少

  + GoogLeNet

    由基础块构成，Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。最后进行通道合并

