+ 过拟合、欠拟合及其应对

  + 训练误差、泛化误差（在训练集外数据集上的误差，由测试误差近似）

  + 验证集（K折）

  + 相关因素：模型复杂度、训练数据集大小

  + 方法

    + 增加模型复杂度（欠拟合）

    + 增加数据（欠拟合）
  
    + L2范数正则化（过拟合）：权重衰减，L2范数惩罚项元素每个参数平方和乘以正的常数

      pytorch实现，`nn.init`初始化参数,`nn.optimm.SGD(xx,xx,weight_decay=true)`启用权重衰减

    + 丢弃法（过拟合）：隐藏层设置概率，隐藏单元p清零,1-p参数拉伸。测试时不使用。

      pytorch实现，`nn.Sequential(..,nn.ReLU(),nn.Dropout(p),...)`

+ 梯度消失、梯度爆炸

  + 层数较多时，容易发生梯度消失或爆炸

  + 随机参数初始化：源代码、Xavier随机初始化

  + 环境因素

    + 协变量偏移：p(x)变化,p(y|x)不变。训练集真实猫狗，测试集卡通猫狗

    + 标签偏移：p(x|y)不变。病因和症状

    + 概念偏移：地理位置不同概念不同

+ RNN进阶

  + 易出现梯度衰减或爆炸，引入门控循环网络

  + GRU：

    重置门捕捉短期依赖，更新门捕捉长期依赖，候选隐藏状态，大小为x×h

    `nn.gru`

  + LSTM（长短期记忆）

    遗忘门、输入门、输出门（均由输入和隐藏状态组成）、记忆细胞

    遗忘门:控制上一时间步的记忆细胞 
    
    输入门:控制当前时间步的输入（候选记忆细胞输入）
    
    输出门:控制从已得到的记忆细胞到隐藏状态

    记忆细胞：⼀种特殊的隐藏状态的信息的流动

    `nn.lstm`

  + 深度循环神经网络

    多个隐藏层，后一隐藏层输入为上一隐藏层输出,不是层数越深越好

    `nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=2)`,`num_layers`控制隐藏层层数

  + 双向循环神经网络

    同时考虑语句中前后对他的考虑

    `nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=True)`，`bidirectional`